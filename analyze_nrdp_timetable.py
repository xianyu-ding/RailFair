import re
import os
import sys
import json
import argparse
import sqlite3
from pathlib import Path
from datetime import datetime, time, date
from collections import defaultdict, Counter
from statistics import median
from typing import Optional, List

def parse_cif_time(time_str):
    """è§£æCIFæ—¶é—´æ ¼å¼ (HHMM æˆ– HHMMH)"""
    if not time_str or time_str.strip() in ('', '0000'):
        return None
    
    time_str = time_str.strip()
    
    # å¤„ç†åŠåˆ†é’Ÿæ ‡è®° (å¦‚ 0841H)
    half_minute = time_str.endswith('H')
    if half_minute:
        time_str = time_str[:-1]
    
    # éªŒè¯æ˜¯å¦ä¸º4ä½æ•°å­—
    if not time_str.isdigit() or len(time_str) != 4:
        return None
    
    try:
        hour = int(time_str[:2])
        minute = int(time_str[2:4])
        
        if hour > 23 or minute > 59:
            return None
            
        return time(hour, minute, 30 if half_minute else 0)
    except (ValueError, IndexError):
        return None

def parse_cif_date(date_str):
    """è§£æCIFæ—¥æœŸæ ¼å¼ (YYMMDD)"""
    if not date_str or len(date_str) != 6:
        return None
    
    try:
        year = int(date_str[0:2]) + 2000  # å‡è®¾æ˜¯21ä¸–çºª
        month = int(date_str[2:4])
        day = int(date_str[4:6])
        return datetime(year, month, day).date()
    except (ValueError, IndexError):
        return None

def parse_ddmmyy_date(date_str):
    """è§£æDDMMYYæ ¼å¼æ—¥æœŸ (ç”¨äºHDè®°å½•çš„Extract dates)"""
    if not date_str or len(date_str) != 6:
        return None
    
    try:
        day = int(date_str[0:2])
        month = int(date_str[2:4])
        year = int(date_str[4:6])
        # å¹´ä»½00-50è¡¨ç¤º2000-2050ï¼Œ51-99è¡¨ç¤º1951-1999
        full_year = 2000 + year if year < 50 else 1900 + year
        return datetime(full_year, month, day).date()
    except (ValueError, IndexError):
        return None

def extract_timetable_date_range(cif_content):
    """
    æ ¹æ®NRDPè§„èŒƒä»timetableæ•°æ®ä¸­æå–æ—¶é—´èŒƒå›´
    
    æ ¹æ®è§„èŒƒæ–‡æ¡£ RSPS5046:
    - HDè®°å½•: ä½ç½®49-54 = Extract start date (ddmmyy), ä½ç½®55-60 = Extract end date (ddmmyy)
    - BSè®°å½•: ä½ç½®10-15 = Date Runs From (yymmdd), ä½ç½®16-21 = Date Runs To (yymmdd)
    """
    date_range_info = {
        'start_date': None,
        'end_date': None,
        'validity_period': None,
        'source': None
    }
    
    all_start_dates = []
    all_end_dates = []
    
    # æŸ¥æ‰¾HDè®°å½•ï¼ˆHeader Recordï¼‰
    for line in cif_content.split('\n'):
        line = line.rstrip('\r\n')
        if len(line) >= 60 and line[0:2] == 'HD':
            # æ ¹æ®è§„èŒƒï¼ŒHDè®°å½•æ ¼å¼ï¼š
            # ä½ç½®49-54: Extract start date (ddmmyy)
            # ä½ç½®55-60: Extract end date (ddmmyy)
            if len(line) >= 60:
                extract_start = line[48:54]  # ä½ç½®49-54 (0-indexed: 48-54)
                extract_end = line[54:60]    # ä½ç½®55-60 (0-indexed: 54-60)
                
                start_date = parse_ddmmyy_date(extract_start)
                end_date = parse_ddmmyy_date(extract_end)
                
                if start_date and end_date:
                    all_start_dates.append(start_date)
                    all_end_dates.append(end_date)
                    date_range_info['source'] = 'HD record (Extract dates)'
    
    # ä»BSè®°å½•ä¸­æå–æ‰€æœ‰æœåŠ¡çš„æ—¥æœŸèŒƒå›´
    bs_dates_found = 0
    for line in cif_content.split('\n'):
        line = line.rstrip('\r\n')
        if len(line) >= 21 and line[0:2] == 'BS':
            # æ ¹æ®è§„èŒƒï¼ŒBSè®°å½•æ ¼å¼ï¼š
            # ä½ç½®10-15: Date Runs From (yymmdd)
            # ä½ç½®16-21: Date Runs To (yymmdd)
            date_from = line[9:15]   # ä½ç½®10-15 (0-indexed: 9-15)
            date_to = line[15:21]    # ä½ç½®16-21 (0-indexed: 15-21)
            
            start_date = parse_cif_date(date_from)
            end_date = parse_cif_date(date_to)
            
            if start_date and end_date:
                all_start_dates.append(start_date)
                all_end_dates.append(end_date)
                bs_dates_found += 1
    
    if bs_dates_found > 0:
        if not date_range_info['source']:
            date_range_info['source'] = f'BS records ({bs_dates_found} services)'
    
    # ç¡®å®šæœ€ç»ˆçš„æ—¶é—´èŒƒå›´
    if all_start_dates and all_end_dates:
        min_start = min(all_start_dates)
        max_end = max(all_end_dates)
        
        date_range_info['start_date'] = min_start.strftime('%Y-%m-%d')
        date_range_info['end_date'] = max_end.strftime('%Y-%m-%d')
        
        days = (max_end - min_start).days + 1
        date_range_info['validity_period'] = f"{days} å¤©"
        
        if not date_range_info['source']:
            date_range_info['source'] = 'BS records'
    
    return date_range_info

def parse_days_run(days_str):
    """è§£æè¿è¡Œæ—¥æœŸæ¨¡å¼ (1111100 = å‘¨ä¸€åˆ°å‘¨äº”)"""
    if not days_str or len(days_str) != 7:
        return []
    
    day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    return [day_names[i] for i, char in enumerate(days_str) if char == '1']

def extract_service_times(cif_content):
    """ä»CIFå†…å®¹ä¸­æå–æœåŠ¡æ—¶é—´"""
    services = []
    current_service = None
    
    for line in cif_content.split('\n'):
        line = line.rstrip('\r\n')
        
        if len(line) < 2:
            continue
            
        record_type = line[0:2]
        
        # BSè®°å½• - åŸºæœ¬æœåŠ¡ä¿¡æ¯
        if record_type == 'BS':
            if len(line) >= 80:
                transaction_type = line[2:3]
                train_uid = line[3:9].strip()
                start_date = line[9:15]
                end_date = line[15:21]
                days = line[21:28]
                train_status = line[29:30]
                train_category = line[30:32]
                train_identity = line[32:36].strip()
                
                current_service = {
                    'transaction_type': transaction_type,
                    'train_uid': train_uid,
                    'start_date': parse_cif_date(start_date),
                    'end_date': parse_cif_date(end_date),
                    'days_run': parse_days_run(days),
                    'train_status': train_status,
                    'train_category': train_category,
                    'train_identity': train_identity,
                    'origin_time': None,
                    'destination_time': None,
                    'origin_location': None,
                    'destination_location': None,
                    'intermediate_stops': []
                }
        
        # BXè®°å½• - æ‰©å±•ä¿¡æ¯
        elif record_type == 'BX' and current_service:
            if len(line) >= 22:
                atoc_code = line[11:13]
                retail_service_id = line[14:22].strip()
                current_service['atoc_code'] = atoc_code
                current_service['retail_service_id'] = retail_service_id
        
        # LOè®°å½• - èµ·ç‚¹ä½ç½®
        elif record_type == 'LO' and current_service:
            if len(line) >= 19:
                location = line[2:10].strip()
                scheduled_dep = line[10:15]
                public_dep = line[15:19]
                platform = line[19:22].strip() if len(line) >= 22 else ''
                
                current_service['origin_location'] = location
                
                # ä¼˜å…ˆä½¿ç”¨publicæ—¶é—´
                time_to_parse = public_dep if public_dep.strip() and public_dep.strip() != '0000' else scheduled_dep
                parsed_time = parse_cif_time(time_to_parse)
                
                if parsed_time:
                    current_service['origin_time'] = parsed_time
        
        # LIè®°å½• - ä¸­é—´ç«™ç‚¹
        elif record_type == 'LI' and current_service:
            if len(line) >= 29:
                location = line[2:10].strip()
                scheduled_arr = line[10:15]
                scheduled_dep = line[15:20]
                scheduled_pass = line[20:25]
                public_arr = line[25:29]
                public_dep = line[29:33] if len(line) >= 33 else ''
                platform = line[33:36].strip() if len(line) >= 36 else ''
                
                # è§£æåˆ°è¾¾æ—¶é—´
                arr_time = None
                if public_arr.strip() and public_arr.strip() != '0000':
                    arr_time = parse_cif_time(public_arr)
                elif scheduled_arr.strip() and scheduled_arr.strip() != '0000':
                    arr_time = parse_cif_time(scheduled_arr)
                
                # è§£æå‡ºå‘æ—¶é—´
                dep_time = None
                if public_dep.strip() and public_dep.strip() != '0000':
                    dep_time = parse_cif_time(public_dep)
                elif scheduled_dep.strip() and scheduled_dep.strip() != '0000':
                    dep_time = parse_cif_time(scheduled_dep)
                
                # è§£æé€šè¿‡æ—¶é—´
                pass_time = None
                if scheduled_pass.strip() and scheduled_pass.strip() != '0000':
                    pass_time = parse_cif_time(scheduled_pass)
                
                stop_info = {
                    'location': location,
                    'arrival': arr_time,
                    'departure': dep_time,
                    'pass': pass_time,
                    'platform': platform
                }
                
                current_service['intermediate_stops'].append(stop_info)
        
        # LTè®°å½• - ç»ˆç‚¹ä½ç½®
        elif record_type == 'LT' and current_service:
            if len(line) >= 19:
                location = line[2:10].strip()
                scheduled_arr = line[10:15]
                public_arr = line[15:19]
                platform = line[19:22].strip() if len(line) >= 22 else ''
                
                current_service['destination_location'] = location
                
                time_to_parse = public_arr if public_arr.strip() and public_arr.strip() != '0000' else scheduled_arr
                parsed_time = parse_cif_time(time_to_parse)
                
                if parsed_time:
                    current_service['destination_time'] = parsed_time
                
                # ä¿å­˜å®Œæ•´çš„æœåŠ¡è®°å½•
                if current_service['origin_time'] or current_service['destination_time']:
                    services.append(current_service.copy())
                
                current_service = None
    
    return services

def save_parsed_data(services, date_range_info, output_file='data/timetable_parsed.json', source_files=None):
    """ä¿å­˜è§£æåçš„æ•°æ®åˆ°JSONæ–‡ä»¶"""
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    range_info = (date_range_info or {}).copy()
    
    if source_files:
        range_info['files'] = [Path(p).name for p in source_files]
        range_info['file_count'] = len(source_files)
    
    # è½¬æ¢æ•°æ®ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
    serializable_services = []
    for service in services:
        serializable_service = {
            'train_uid': service['train_uid'],
            'atoc_code': service.get('atoc_code', ''),
            'retail_service_id': service.get('retail_service_id', ''),
            'start_date': service['start_date'].isoformat() if service['start_date'] else None,
            'end_date': service['end_date'].isoformat() if service['end_date'] else None,
            'days_run': service['days_run'],
            'train_category': service['train_category'],
            'origin_location': service['origin_location'],
            'origin_time': service['origin_time'].isoformat() if service['origin_time'] else None,
            'destination_location': service['destination_location'],
            'destination_time': service['destination_time'].isoformat() if service['destination_time'] else None,
            'intermediate_stops_count': len(service['intermediate_stops'])
        }
        serializable_services.append(serializable_service)
    
    data = {
        'parsed_at': datetime.now().isoformat(),
        'date_range': range_info,
        'total_services': len(services),
        'services': serializable_services
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    return output_path

def load_parsed_data(input_file='data/timetable_parsed.json'):
    """ä»JSONæ–‡ä»¶åŠ è½½å·²è§£æçš„æ•°æ®"""
    input_path = Path(input_file)
    if not input_path.exists():
        return None, None
    
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # è½¬æ¢å›åŸå§‹æ ¼å¼
    services = []
    for s in data['services']:
        # è§£ææ—¥æœŸ
        start_date = None
        if s['start_date']:
            try:
                start_date = datetime.fromisoformat(s['start_date']).date()
            except (ValueError, TypeError):
                # å°è¯•å…¶ä»–æ—¥æœŸæ ¼å¼
                try:
                    start_date = datetime.strptime(s['start_date'], '%Y-%m-%d').date()
                except (ValueError, TypeError):
                    pass
        
        end_date = None
        if s['end_date']:
            try:
                end_date = datetime.fromisoformat(s['end_date']).date()
            except (ValueError, TypeError):
                try:
                    end_date = datetime.strptime(s['end_date'], '%Y-%m-%d').date()
                except (ValueError, TypeError):
                    pass
        
        # è§£ææ—¶é—´ï¼ˆä½¿ç”¨time.fromisoformatæˆ–æ‰‹åŠ¨è§£æï¼‰
        origin_time = None
        if s['origin_time']:
            try:
                origin_time = time.fromisoformat(s['origin_time'])
            except (ValueError, TypeError):
                try:
                    # æ‰‹åŠ¨è§£æ HH:MM:SS æ ¼å¼
                    parts = s['origin_time'].split(':')
                    if len(parts) >= 2:
                        origin_time = time(int(parts[0]), int(parts[1]), int(parts[2]) if len(parts) > 2 else 0)
                except (ValueError, TypeError, IndexError):
                    pass
        
        destination_time = None
        if s['destination_time']:
            try:
                destination_time = time.fromisoformat(s['destination_time'])
            except (ValueError, TypeError):
                try:
                    # æ‰‹åŠ¨è§£æ HH:MM:SS æ ¼å¼
                    parts = s['destination_time'].split(':')
                    if len(parts) >= 2:
                        destination_time = time(int(parts[0]), int(parts[1]), int(parts[2]) if len(parts) > 2 else 0)
                except (ValueError, TypeError, IndexError):
                    pass
        
        service = {
            'train_uid': s['train_uid'],
            'atoc_code': s.get('atoc_code', ''),
            'retail_service_id': s.get('retail_service_id', ''),
            'start_date': start_date,
            'end_date': end_date,
            'days_run': s['days_run'],
            'train_category': s['train_category'],
            'origin_location': s['origin_location'],
            'origin_time': origin_time,
            'destination_location': s['destination_location'],
            'destination_time': destination_time,
            'intermediate_stops': [],
            'intermediate_stops_count': s.get('intermediate_stops_count', 0)  # ç¡®ä¿åŒ…å«ä¸­é—´ç«™ç‚¹æ•°é‡
        }
        services.append(service)
    
    return services, data.get('date_range', {})

def parse_iso_date(value: str) -> Optional[date]:
    """Parse YYYY-MM-DD strings into date objects"""
    if not value:
        return None
    try:
        return datetime.strptime(value, '%Y-%m-%d').date()
    except ValueError:
        return None

def get_cache_age_days(parsed_data: dict) -> Optional[int]:
    """Return cache age in days if parsed_at is present"""
    parsed_at = parsed_data.get('parsed_at')
    if not parsed_at:
        return None
    try:
        parsed_dt = datetime.fromisoformat(parsed_at)
    except ValueError:
        try:
            parsed_dt = datetime.strptime(parsed_at, '%Y-%m-%dT%H:%M:%S')
        except ValueError:
            return None
    delta = datetime.now() - parsed_dt
    return delta.days

def parse_date_string(value: Optional[str], label: str = "æ—¥æœŸ") -> Optional[date]:
    """Parse CLI date string in YYYY-MM-DD format"""
    if not value:
        return None
    try:
        return datetime.strptime(value, '%Y-%m-%d').date()
    except ValueError:
        print(f"âœ— æ— æ•ˆçš„{label}: {value} (è¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼)")
        sys.exit(1)

def filter_services_by_date_range(services, start_date: Optional[date] = None, end_date: Optional[date] = None):
    """Filter services within optional [start_date, end_date]"""
    if not start_date and not end_date:
        return services
    
    filtered = []
    for service in services:
        svc_start = service.get('start_date')
        svc_end = service.get('end_date') or svc_start
        
        if start_date and svc_end and svc_end < start_date:
            continue
        if end_date and svc_start and svc_start > end_date:
            continue
        filtered.append(service)
    
    return filtered

def calculate_duration_minutes(service) -> Optional[int]:
    """Calculate duration in minutes between origin and destination times"""
    origin_time = service.get('origin_time')
    destination_time = service.get('destination_time')
    
    if not origin_time or not destination_time:
        return None
    
    start_minutes = origin_time.hour * 60 + origin_time.minute + origin_time.second / 60
    end_minutes = destination_time.hour * 60 + destination_time.minute + destination_time.second / 60
    
    duration = end_minutes - start_minutes
    if duration < 0:
        duration += 24 * 60  # è·¨åˆå¤œ
    
    return round(duration)

def aggregate_route_metrics(services):
    """Aggregate timetable services into route-level metrics"""
    route_stats = {}
    
    for service in services:
        origin = service.get('origin_location')
        destination = service.get('destination_location')
        if not origin or not destination:
            continue
        
        key = (origin, destination)
        if key not in route_stats:
            route_stats[key] = {
                'origin': origin,
                'destination': destination,
                'total_services': 0,
                'start_date': None,
                'end_date': None,
                'durations': [],
                'hour_counter': Counter(),
                'total_stops': 0,
                'stop_samples': 0
            }
        
        entry = route_stats[key]
        entry['total_services'] += 1
        
        duration = calculate_duration_minutes(service)
        if duration is not None:
            entry['durations'].append(duration)
        
        svc_start = service.get('start_date')
        svc_end = service.get('end_date') or svc_start
        
        if svc_start and (entry['start_date'] is None or svc_start < entry['start_date']):
            entry['start_date'] = svc_start
        if svc_end and (entry['end_date'] is None or svc_end > entry['end_date']):
            entry['end_date'] = svc_end
        
        origin_time = service.get('origin_time')
        if isinstance(origin_time, time):
            entry['hour_counter'][origin_time.hour] += 1
        
        stops = service.get('intermediate_stops_count')
        if isinstance(stops, int):
            entry['total_stops'] += stops
            entry['stop_samples'] += 1
    
    aggregated = []
    for (origin, destination), entry in route_stats.items():
        durations = entry['durations']
        typical_duration = round(median(durations)) if durations else None
        avg_stops = entry['total_stops'] / entry['stop_samples'] if entry['stop_samples'] else 0
        
        start_date = entry['start_date']
        end_date = entry['end_date']
        total_days = (end_date - start_date).days + 1 if start_date and end_date else None
        services_per_day = entry['total_services'] / total_days if total_days else None
        
        if services_per_day:
            service_frequency = f"{services_per_day:.1f} per day"
        else:
            service_frequency = f"{entry['total_services']} services"
        
        peak_hours = [f"{hour:02d}:00" for hour, _ in entry['hour_counter'].most_common(3)]
        peak_times = ", ".join(peak_hours)
        
        if typical_duration and typical_duration >= 150:
            route_type = "long_distance"
        elif avg_stops >= 8:
            route_type = "intercity"
        else:
            route_type = "regional"
        
        if services_per_day and services_per_day >= 8:
            priority_tier = 1
        elif services_per_day and services_per_day >= 4:
            priority_tier = 2
        else:
            priority_tier = 3
        
        coverage_note = None
        if start_date and end_date:
            coverage_note = f"Timetable coverage {start_date} â†’ {end_date} ({entry['total_services']} services)"
        
        aggregated.append({
            "route_id": f"{origin}-{destination}",
            "origin": origin,
            "destination": destination,
            "typical_duration_minutes": typical_duration,
            "service_frequency": service_frequency,
            "peak_times": peak_times,
            "route_type": route_type,
            "priority_tier": priority_tier,
            "notes": coverage_note,
            "start_date": start_date,
            "end_date": end_date
        })
    
    return aggregated

def sync_route_metadata_to_db(route_records, db_path: str):
    """Persist aggregated route metrics into route_metadata table"""
    if not route_records:
        print("âš ï¸ æ²¡æœ‰å¯åŒæ­¥çš„è·¯çº¿æ•°æ®")
        return
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # ç¡®ä¿è¡¨å­˜åœ¨ï¼ˆä¸collect_metadataä¿æŒä¸€è‡´ï¼‰
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS route_metadata (
            route_id TEXT PRIMARY KEY,
            origin_crs TEXT NOT NULL,
            destination_crs TEXT NOT NULL,
            distance_km REAL,
            typical_duration_minutes INTEGER,
            service_frequency TEXT,
            peak_times TEXT,
            route_type TEXT,
            priority_tier INTEGER DEFAULT 3,
            notes TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    for record in route_records:
        cursor.execute("""
            INSERT INTO route_metadata (
                route_id,
                origin_crs,
                destination_crs,
                typical_duration_minutes,
                service_frequency,
                peak_times,
                route_type,
                priority_tier,
                notes
            )
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(route_id) DO UPDATE SET
                origin_crs=excluded.origin_crs,
                destination_crs=excluded.destination_crs,
                typical_duration_minutes=excluded.typical_duration_minutes,
                service_frequency=excluded.service_frequency,
                peak_times=excluded.peak_times,
                route_type=excluded.route_type,
                priority_tier=excluded.priority_tier,
                notes=excluded.notes,
                updated_at=CURRENT_TIMESTAMP
        """, (
            record["route_id"],
            record["origin"],
            record["destination"],
            record["typical_duration_minutes"],
            record["service_frequency"],
            record["peak_times"],
            record["route_type"],
            record["priority_tier"],
            record["notes"]
        ))
    
    conn.commit()
    conn.close()
    print(f"ğŸ’¾ å·²åŒæ­¥ {len(route_records)} æ¡è·¯çº¿åˆ° route_metadata ï¼ˆæ•°æ®åº“: {db_path}ï¼‰")

def analyze_services(services):
    """åˆ†ææœåŠ¡æ•°æ®"""
    print("\n" + "="*80)
    print("æœåŠ¡åˆ†æç»Ÿè®¡")
    print("="*80)
    
    # æŒ‰TOCç»Ÿè®¡
    toc_stats = defaultdict(int)
    route_stats = defaultdict(int)
    
    for service in services:
        toc = service.get('atoc_code', 'Unknown')
        toc_stats[toc] += 1
        
        origin = service.get('origin_location', 'Unknown')
        dest = service.get('destination_location', 'Unknown')
        route = f"{origin} â†’ {dest}"
        route_stats[route] += 1
    
    print(f"\næ€»æœåŠ¡æ•°: {len(services)}")
    
    print("\næŒ‰è¿è¥å•†ç»Ÿè®¡:")
    for toc, count in sorted(toc_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"  {toc}: {count} ä¸ªæœåŠ¡")
    
    print("\nçƒ­é—¨è·¯çº¿ (å‰10):")
    for route, count in sorted(route_stats.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  {route}: {count} ä¸ªæœåŠ¡")
    
    # æ—¶é—´æ®µåˆ†æ
    hour_stats = defaultdict(int)
    for service in services:
        if service.get('origin_time'):
            hour = service['origin_time'].hour
            hour_stats[hour] += 1
    
    print("\nå‡ºå‘æ—¶é—´åˆ†å¸ƒ (æŒ‰å°æ—¶):")
    for hour in sorted(hour_stats.keys()):
        count = hour_stats[hour]
        bar = 'â–ˆ' * (count // 5)  # æ¯ä¸ªâ–ˆä»£è¡¨5ä¸ªæœåŠ¡
        print(f"  {hour:02d}:00 - {hour:02d}:59 | {bar} ({count})")

def main():
    parser = argparse.ArgumentParser(
        description='NRDP Timetable æ•°æ®åˆ†æå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä»æœ¬åœ°ç›®å½•è¯»å–MCAæ–‡ä»¶
  python analyze_nrdp_timetable.py --dir data/nrdp_timetable
  
  # ä»æŒ‡å®šçš„MCAæ–‡ä»¶è¯»å–
  python analyze_nrdp_timetable.py --file data/nrdp_timetable/RJTTF652.MCA
        """
    )
    parser.add_argument(
        '--dir',
        type=str,
        help='ä»æœ¬åœ°ç›®å½•è¯»å–MCAæ–‡ä»¶ï¼ˆä¼šè‡ªåŠ¨æŸ¥æ‰¾.MCAæ–‡ä»¶ï¼‰'
    )
    parser.add_argument(
        '--file',
        type=str,
        help='ä»æŒ‡å®šçš„MCAæ–‡ä»¶è¯»å–'
    )
    parser.add_argument(
        '--use-cached',
        action='store_true',
        help='ä¼˜å…ˆä½¿ç”¨å·²ä¿å­˜çš„è§£ææ•°æ®ï¼Œå³ä¾¿è¶…è¿‡ç¼“å­˜æœ‰æ•ˆæœŸ'
    )
    parser.add_argument(
        '--save',
        action='store_true',
        default=True,
        help='ä¿å­˜è§£æåçš„æ•°æ®ï¼ˆé»˜è®¤å¼€å¯ï¼‰'
    )
    parser.add_argument(
        '--cache-ttl-days',
        type=int,
        default=30,
        help='ç¼“å­˜æœ‰æ•ˆå¤©æ•°ï¼Œé»˜è®¤30å¤©'
    )
    parser.add_argument(
        '--force-parse',
        action='store_true',
        help='å¿½ç•¥ç¼“å­˜å¼ºåˆ¶é‡æ–°è§£æ'
    )
    parser.add_argument(
        '--start-date',
        type=str,
        help='ä»…ä¿ç•™è¯¥æ—¥æœŸï¼ˆå«ï¼‰ä¹‹åçš„æœåŠ¡ï¼Œæ ¼å¼ YYYY-MM-DD'
    )
    parser.add_argument(
        '--end-date',
        type=str,
        help='ä»…ä¿ç•™è¯¥æ—¥æœŸï¼ˆå«ï¼‰ä¹‹å‰çš„æœåŠ¡ï¼Œæ ¼å¼ YYYY-MM-DD'
    )
    parser.add_argument(
        '--sync-db',
        action='store_true',
        help='å°†èšåˆåçš„è·¯çº¿æŒ‡æ ‡å†™å…¥ route_metadata è¡¨'
    )
    parser.add_argument(
        '--db-path',
        type=str,
        default='data/railfair.db',
        help='SQLite æ•°æ®åº“è·¯å¾„ï¼ˆé»˜è®¤: data/railfair.dbï¼‰'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å·²ä¿å­˜çš„æ•°æ®
    parsed_data_file = Path('data/timetable_parsed.json')
    source_files: List[str] = []
    services = None
    date_range_info = None
    
    cache_metadata = None
    cache_age_days = None
    if parsed_data_file.exists():
        try:
            with open(parsed_data_file, 'r', encoding='utf-8') as f:
                cache_metadata = json.load(f)
                cache_age_days = get_cache_age_days(cache_metadata)
        except Exception:
            cache_metadata = None
            cache_age_days = None
    
    use_cache = False
    if cache_metadata and not args.force_parse:
        if args.use_cached:
            use_cache = True
        elif cache_age_days is not None and cache_age_days <= args.cache_ttl_days:
            use_cache = True
    
    if use_cache and parsed_data_file.exists():
        if cache_age_days is not None:
            print(f"ğŸ“‚ ç¼“å­˜å‘½ä¸­: {parsed_data_file} ï¼ˆ{cache_age_days} å¤©å‰ç”Ÿæˆï¼‰")
        else:
            print(f"ğŸ“‚ ä½¿ç”¨ç¼“å­˜: {parsed_data_file}")
        services, date_range_info = load_parsed_data(str(parsed_data_file))
        if services:
            print(f"âœ“ åŠ è½½æˆåŠŸ: {len(services):,} ä¸ªæœåŠ¡")
            if isinstance(date_range_info, dict):
                source_files = date_range_info.get('files', [])
        else:
            print("âœ— ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°è§£æ")
            services = None
            date_range_info = None
    elif cache_metadata and not args.force_parse and cache_age_days is not None and cache_age_days > args.cache_ttl_days:
        print(f"âš ï¸ ç¼“å­˜å·²è¶…è¿‡ {args.cache_ttl_days} å¤©ï¼ˆå½“å‰ {cache_age_days} å¤©ï¼‰ï¼Œå°†é‡æ–°è§£æ")
    
    # å¦‚æœéœ€è¦é‡æ–°è§£æ
    if services is None:
        cif_file_paths: List[Path] = []
        
        if args.dir:
            dir_path = Path(args.dir)
            if not dir_path.exists():
                print(f"âœ— é”™è¯¯: ç›®å½•ä¸å­˜åœ¨ {dir_path}")
                return
            mca_files = list(dir_path.glob('*.MCA')) + list(dir_path.glob('*.mca'))
            if not mca_files:
                print(f"âœ— é”™è¯¯: åœ¨ç›®å½• {dir_path} ä¸­æœªæ‰¾åˆ°MCAæ–‡ä»¶")
                return
            cif_file_paths = sorted(mca_files)
            print(f"ğŸ“‚ ä»ç›®å½•è¯»å–: {dir_path}")
            print(f"   æ‰¾åˆ° {len(cif_file_paths)} ä¸ªMCAæ–‡ä»¶")
        
        elif args.file:
            cif_file_path = Path(args.file)
            if not cif_file_path.exists():
                print(f"âœ— é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {cif_file_path}")
                return
            cif_file_paths = [cif_file_path]
        else:
            default_dir = Path('data/nrdp_timetable')
            if default_dir.exists():
                mca_files = list(default_dir.glob('*.MCA')) + list(default_dir.glob('*.mca'))
                if mca_files:
                    cif_file_paths = sorted(mca_files)
                    print(f"ğŸ“‚ ä½¿ç”¨é»˜è®¤ç›®å½•: {default_dir}")
                    print(f"   æ‰¾åˆ° {len(cif_file_paths)} ä¸ªMCAæ–‡ä»¶")
                else:
                    print("âœ— é”™è¯¯: è¯·ä½¿ç”¨ --dir æˆ– --file å‚æ•°æŒ‡å®šæ–‡ä»¶è·¯å¾„")
                    parser.print_help()
                    return
            else:
                print("âœ— é”™è¯¯: è¯·ä½¿ç”¨ --dir æˆ– --file å‚æ•°æŒ‡å®šæ–‡ä»¶è·¯å¾„")
                parser.print_help()
                return
        
        if not cif_file_paths:
            print("âœ— æœªæ‰¾åˆ°ä»»ä½•MCAæ–‡ä»¶")
            return
        
        services = []
        combined_start = None
        combined_end = None
        combined_sources = set()
        
        for idx, cif_file_path in enumerate(cif_file_paths, start=1):
            print(f"\n[{idx}/{len(cif_file_paths)}] æ­£åœ¨è¯»å–CIFæ–‡ä»¶: {cif_file_path}")
            try:
                with open(cif_file_path, 'r', encoding='latin-1', errors='ignore') as f:
                    cif_content = f.read()
                print(f"âœ“ æ–‡ä»¶è¯»å–æˆåŠŸï¼Œå¤§å°: {len(cif_content):,} å­—èŠ‚")
            except FileNotFoundError:
                print(f"âœ— é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {cif_file_path}")
                return
            except Exception as e:
                print(f"âœ— è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                return
            
            print("æ­£åœ¨è§£æCIFæ•°æ®...")
            file_date_range = extract_timetable_date_range(cif_content)
            file_services = extract_service_times(cif_content)
            services.extend(file_services)
            source_files.append(str(cif_file_path))
            
            file_start = parse_iso_date(file_date_range.get('start_date'))
            file_end = parse_iso_date(file_date_range.get('end_date'))
            if file_start and (combined_start is None or file_start < combined_start):
                combined_start = file_start
            if file_end and (combined_end is None or file_end > combined_end):
                combined_end = file_end
            if file_date_range.get('source'):
                combined_sources.add(file_date_range['source'])
            
            print(f"   è§£æå®Œæˆ: {len(file_services):,} ä¸ªæœåŠ¡ï¼ˆç´¯è®¡ {len(services):,}ï¼‰")
        
        validity_days = (combined_end - combined_start).days + 1 if combined_start and combined_end else None
        date_range_info = {
            'start_date': combined_start.strftime('%Y-%m-%d') if combined_start else None,
            'end_date': combined_end.strftime('%Y-%m-%d') if combined_end else None,
            'validity_period': f"{validity_days} å¤©" if validity_days else None,
            'source': " | ".join(sorted(combined_sources)) if combined_sources else None,
            'file_count': len(source_files),
            'files': [Path(p).name for p in source_files]
        }
        
        if args.save:
            saved_path = save_parsed_data(services, date_range_info, source_files=source_files)
            print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {saved_path}")
    
    # æ ¹æ®éœ€è¦è¿‡æ»¤æ—¥æœŸ
    filter_start = parse_date_string(args.start_date, "å¼€å§‹æ—¥æœŸ") if args.start_date else None
    filter_end = parse_date_string(args.end_date, "ç»“æŸæ—¥æœŸ") if args.end_date else None
    if filter_start and filter_end and filter_start > filter_end:
        print("âœ— å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸ")
        return
    
    filter_labels = []
    if filter_start:
        filter_labels.append(f">= {filter_start}")
    if filter_end:
        filter_labels.append(f"<= {filter_end}")
    active_filter_label = " & ".join(filter_labels) if filter_labels else "å…¨éƒ¨è§£æèŒƒå›´"

    original_count = len(services)
    services = filter_services_by_date_range(services, filter_start, filter_end)
    
    if original_count != len(services):
        print(f"ğŸ” å·²è¿‡æ»¤: {original_count:,} â†’ {len(services):,} ä¸ªæœåŠ¡ï¼ˆ{active_filter_label}ï¼‰")
    
    # åˆ†ææœåŠ¡æ•°æ®
    print("æ­£åœ¨åˆ†ææ•°æ®...")
    toc_stats = defaultdict(int)
    route_stats = defaultdict(int)  # è·¯çº¿é¢‘ç‡ç»Ÿè®¡
    route_distance_stats = defaultdict(lambda: {'count': 0, 'total_stops': 0, 'avg_stops': 0})  # è·¯çº¿è·ç¦»ç»Ÿè®¡ï¼ˆæŒ‰ä¸­é—´ç«™ç‚¹æ•°ï¼‰
    hour_stats = defaultdict(int)
    
    for service in services:
        toc = service.get('atoc_code', 'Unknown')
        toc_stats[toc] += 1
        
        origin = service.get('origin_location', 'Unknown')
        dest = service.get('destination_location', 'Unknown')
        route = f"{origin} â†’ {dest}"
        route_stats[route] += 1
        
        # è®¡ç®—ä¸­é—´ç«™ç‚¹æ•°é‡ï¼ˆç”¨äºåˆ¤æ–­é•¿é€”è·¯çº¿ï¼‰
        intermediate_stops_count = service.get('intermediate_stops_count', 0)
        if route not in route_distance_stats:
            route_distance_stats[route] = {'count': 0, 'total_stops': 0, 'avg_stops': 0}
        route_distance_stats[route]['count'] += 1
        route_distance_stats[route]['total_stops'] += intermediate_stops_count
        
        if service.get('origin_time'):
            hour = service['origin_time'].hour
            hour_stats[hour] += 1
    
    # è®¡ç®—å¹³å‡ä¸­é—´ç«™ç‚¹æ•°
    for route in route_distance_stats:
        if route_distance_stats[route]['count'] > 0:
            route_distance_stats[route]['avg_stops'] = route_distance_stats[route]['total_stops'] / route_distance_stats[route]['count']
    
    # æ˜¾ç¤ºæ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“Š åˆ†ææ€»ç»“")
    print("="*80)
    print(f"\nğŸ“ æ•°æ®èŒƒå›´: {active_filter_label}")
    
    # æ—¶é—´èŒƒå›´ä¿¡æ¯
    if date_range_info['start_date'] and date_range_info['end_date']:
        print(f"\nğŸ“… æ—¶åˆ»è¡¨æœ‰æ•ˆæœŸ:")
        print(f"   å¼€å§‹æ—¥æœŸ: {date_range_info['start_date']}")
        print(f"   ç»“æŸæ—¥æœŸ: {date_range_info['end_date']}")
        print(f"   æœ‰æ•ˆæœŸ: {date_range_info['validity_period']}")
        
        today = datetime.now().date()
        end_date = datetime.strptime(date_range_info['end_date'], '%Y-%m-%d').date()
        if end_date < today:
            days_old = (today - end_date).days
            print(f"   çŠ¶æ€: âš ï¸ å·²è¿‡æœŸ {days_old} å¤©")
        elif end_date >= today:
            days_remaining = (end_date - today).days
            print(f"   çŠ¶æ€: âœ… æœ‰æ•ˆï¼ˆå‰©ä½™ {days_remaining} å¤©ï¼‰")
    else:
        print(f"\nğŸ“… æ—¶åˆ»è¡¨æœ‰æ•ˆæœŸ: âš ï¸ æœªèƒ½æå–")
    
    # æœåŠ¡ç»Ÿè®¡
    print(f"\nğŸš‚ æœåŠ¡ç»Ÿè®¡:")
    print(f"   æ€»æœåŠ¡æ•°: {len(services):,}")
    print(f"   è¿è¥å•†æ•°: {len(toc_stats)}")
    print(f"   è·¯çº¿æ•°: {len(route_stats)}")
    
    # Topè¿è¥å•†
    print(f"\nğŸ“ˆ Top 5 è¿è¥å•†:")
    for toc, count in sorted(toc_stats.items(), key=lambda x: x[1], reverse=True)[:5]:
        print(f"   {toc}: {count:,} ä¸ªæœåŠ¡")
    
    # Topè·¯çº¿ï¼ˆæŒ‰é¢‘ç‡ï¼‰
    print(f"\nğŸ›¤ï¸  Top 20 çƒ­é—¨è·¯çº¿ (æŒ‰æœåŠ¡é¢‘ç‡):")
    for idx, (route, count) in enumerate(sorted(route_stats.items(), key=lambda x: x[1], reverse=True)[:20], 1):
        avg_stops = route_distance_stats.get(route, {}).get('avg_stops', 0)
        print(f"   {idx:2d}. {route}: {count:,} ä¸ªæœåŠ¡ (å¹³å‡ {avg_stops:.1f} ä¸ªä¸­é—´ç«™ç‚¹)")
    
    # Topé•¿é€”è·¯çº¿ï¼ˆæŒ‰ä¸­é—´ç«™ç‚¹æ•°ï¼‰
    print(f"\nğŸš„ Top 20 é•¿é€”è·¯çº¿ (æŒ‰å¹³å‡ä¸­é—´ç«™ç‚¹æ•°):")
    # åªè€ƒè™‘è‡³å°‘æœ‰10ä¸ªæœåŠ¡çš„è·¯çº¿ï¼Œé¿å…å•æ¬¡æœåŠ¡çš„å½±å“
    long_distance_routes = [
        (route, stats['avg_stops'], stats['count'])
        for route, stats in route_distance_stats.items()
        if stats['count'] >= 10 and stats['avg_stops'] > 0
    ]
    long_distance_routes.sort(key=lambda x: x[1], reverse=True)  # æŒ‰å¹³å‡ç«™ç‚¹æ•°æ’åº
    
    for idx, (route, avg_stops, count) in enumerate(long_distance_routes[:20], 1):
        print(f"   {idx:2d}. {route}: å¹³å‡ {avg_stops:.1f} ä¸ªä¸­é—´ç«™ç‚¹ ({count:,} ä¸ªæœåŠ¡)")
    
    # Topä¸­é¢‘çƒ­é—¨è·¯çº¿ï¼ˆçƒ­é—¨ä½†éè¶…é•¿é€”ï¼‰
    print(f"\nâ­ Top 20 ä¸­é¢‘çƒ­é—¨è·¯çº¿ (çƒ­é—¨ä½†éè¶…é•¿é€”):")
    # è¿‡æ»¤æ¡ä»¶ï¼šå¹³å‡ä¸­é—´ç«™ç‚¹æ•° <= 20ï¼ˆæ’é™¤è¶…é•¿é€”ï¼‰ï¼ŒæœåŠ¡æ•° >= 20ï¼ˆç¡®ä¿çƒ­é—¨ï¼‰
    medium_distance_routes = [
        (route, count, route_distance_stats.get(route, {}).get('avg_stops', 0))
        for route, count in route_stats.items()
        if count >= 20 and route_distance_stats.get(route, {}).get('avg_stops', 0) <= 20
    ]
    medium_distance_routes.sort(key=lambda x: x[1], reverse=True)  # æŒ‰æœåŠ¡é¢‘ç‡æ’åº
    
    for idx, (route, count, avg_stops) in enumerate(medium_distance_routes[:20], 1):
        print(f"   {idx:2d}. {route}: {count:,} ä¸ªæœåŠ¡ (å¹³å‡ {avg_stops:.1f} ä¸ªä¸­é—´ç«™ç‚¹)")
    
    # å‡ºå‘æ—¶é—´åˆ†å¸ƒï¼ˆåªæ˜¾ç¤ºé«˜å³°æ—¶æ®µï¼‰
    if hour_stats:
        print(f"\nâ° å‡ºå‘æ—¶é—´åˆ†å¸ƒ (é«˜å³°æ—¶æ®µ):")
        peak_hours = sorted([h for h in hour_stats.keys() if 6 <= h <= 22])
        for hour in peak_hours[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            count = hour_stats[hour]
            bar = 'â–ˆ' * min(count // 50, 20)  # æ¯ä¸ªâ–ˆä»£è¡¨50ä¸ªæœåŠ¡ï¼Œæœ€å¤š20ä¸ª
            print(f"   {hour:02d}:00 - {hour:02d}:59 | {bar} {count:,}")
    
    if args.sync_db:
        print("\nğŸ”„ æ­£åœ¨åŒæ­¥è·¯çº¿å…ƒæ•°æ®...")
        route_records = aggregate_route_metrics(services)
        sync_route_metadata_to_db(route_records, args.db_path)
    
    print("\n" + "="*80)

if __name__ == "__main__":
    main()