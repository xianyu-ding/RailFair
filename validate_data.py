#!/usr/bin/env python3
"""
Day 5: æ•°æ®éªŒè¯ä¸»è„šæœ¬
ç”¨äºéªŒè¯å·²æ”¶é›†çš„HSPæ•°æ®è´¨é‡ï¼Œäº¤å‰éªŒè¯ï¼Œç”Ÿæˆç»¼åˆæŠ¥å‘Š
"""

import sqlite3
import json
import argparse
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from pathlib import Path
import statistics
from typing import Dict, List, Tuple, Optional, Any

class DataValidator:
    """ç»¼åˆæ•°æ®éªŒè¯å™¨"""
    
    def __init__(self, db_path: str = "data/railfair.db"):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path)
        self.conn.row_factory = sqlite3.Row
        self.cursor = self.conn.cursor()
        self.validation_results = {
            "summary": {},
            "quality_checks": {},
            "warnings": [],
            "errors": [],
            "recommendations": []
        }
        
    def run_all_validations(self) -> Dict:
        """è¿è¡Œæ‰€æœ‰éªŒè¯æ£€æŸ¥"""
        print("ğŸ” Starting comprehensive data validation...")
        print("=" * 60)
        
        # 1. åŸºç¡€æ•°æ®ç»Ÿè®¡
        self._validate_basic_stats()
        
        # 2. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        self._validate_data_completeness()
        
        # 3. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
        self._validate_data_consistency()
        
        # 4. å»¶è¯¯æ•°æ®è´¨é‡
        self._validate_delay_quality()
        
        # 5. PPM (Public Performance Measure) è®¡ç®—
        self._validate_ppm()
        
        # 6. è·¯çº¿è¦†ç›–æ£€æŸ¥
        self._validate_route_coverage()
        
        # 6. æ—¶é—´åˆ†å¸ƒæ£€æŸ¥
        self._validate_temporal_distribution()
        
        # 7. TOCæ•°æ®éªŒè¯
        self._validate_toc_data()
        
        # 8. å¼‚å¸¸å€¼æ£€æµ‹
        self._detect_anomalies()
        
        # 9. æ•°æ®æ–°é²œåº¦æ£€æŸ¥
        self._validate_data_freshness()
        
        # 10. ç”Ÿæˆå»ºè®®
        self._generate_recommendations()
        
        return self.validation_results
    
    def _validate_basic_stats(self):
        """åŸºç¡€æ•°æ®ç»Ÿè®¡"""
        print("\nğŸ“Š Basic Statistics:")
        print("-" * 40)
        
        # Metricsè¡¨ç»Ÿè®¡
        metrics_count = self.cursor.execute(
            "SELECT COUNT(*) FROM hsp_service_metrics"
        ).fetchone()[0]
        
        # Detailsè¡¨ç»Ÿè®¡
        details_count = self.cursor.execute(
            "SELECT COUNT(*) FROM hsp_service_details"
        ).fetchone()[0]
        
        # å”¯ä¸€è·¯çº¿æ•°
        unique_routes = self.cursor.execute("""
            SELECT COUNT(DISTINCT origin || '-' || destination) 
            FROM hsp_service_metrics
        """).fetchone()[0]
        
        # å”¯ä¸€RIDæ•°
        unique_rids = self.cursor.execute(
            "SELECT COUNT(DISTINCT rid) FROM hsp_service_details"
        ).fetchone()[0]
        
        # æ—¥æœŸèŒƒå›´
        date_range = self.cursor.execute("""
            SELECT MIN(date_of_service), MAX(date_of_service)
            FROM hsp_service_details
        """).fetchone()
        
        # TOCæ•°é‡
        toc_count = self.cursor.execute(
            "SELECT COUNT(DISTINCT toc_code) FROM hsp_service_details"
        ).fetchone()[0]
        
        # ç«™ç‚¹æ•°é‡
        location_count = self.cursor.execute(
            "SELECT COUNT(DISTINCT location) FROM hsp_service_details"
        ).fetchone()[0]
        
        self.validation_results["summary"] = {
            "metrics_records": metrics_count,
            "details_records": details_count,
            "unique_routes": unique_routes,
            "unique_services": unique_rids,
            "date_range": {
                "start": date_range[0],
                "end": date_range[1]
            },
            "toc_count": toc_count,
            "location_count": location_count
        }
        
        print(f"âœ… Metrics records: {metrics_count:,}")
        print(f"âœ… Details records: {details_count:,}")
        print(f"âœ… Unique routes: {unique_routes}")
        print(f"âœ… Unique services (RIDs): {unique_rids:,}")
        print(f"âœ… Date range: {date_range[0]} to {date_range[1]}")
        print(f"âœ… TOCs: {toc_count}")
        print(f"âœ… Locations: {location_count}")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°Week 1ç›®æ ‡
        if details_count >= 10000:
            print(f"ğŸ¯ Week 1 target achieved: {details_count:,} > 10,000 âœ…")
        else:
            print(f"âš ï¸ Below Week 1 target: {details_count:,} < 10,000")
            self.validation_results["warnings"].append(
                f"Data volume below target: {details_count} < 10,000"
            )
    
    def _validate_data_completeness(self):
        """æ•°æ®å®Œæ•´æ€§æ£€æŸ¥"""
        print("\nğŸ” Data Completeness Check:")
        print("-" * 40)
        
        # æ£€æŸ¥NULLå€¼
        null_checks = [
            ("scheduled_departure", "hsp_service_details"),
            ("scheduled_arrival", "hsp_service_details"),
            ("actual_departure", "hsp_service_details"),
            ("actual_arrival", "hsp_service_details"),
            ("departure_delay_minutes", "hsp_service_details"),
            ("arrival_delay_minutes", "hsp_service_details")
        ]
        
        completeness = {}
        for field, table in null_checks:
            total = self.cursor.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0]
            non_null = self.cursor.execute(
                f"SELECT COUNT(*) FROM {table} WHERE {field} IS NOT NULL"
            ).fetchone()[0]
            
            if total > 0:
                percentage = (non_null / total) * 100
                completeness[field] = {
                    "non_null": non_null,
                    "total": total,
                    "percentage": round(percentage, 2)
                }
                
                status = "âœ…" if percentage > 80 else "âš ï¸" if percentage > 50 else "âŒ"
                print(f"{status} {field}: {percentage:.1f}% complete ({non_null:,}/{total:,})")
                
                if percentage < 50:
                    self.validation_results["warnings"].append(
                        f"Low completeness for {field}: {percentage:.1f}%"
                    )
        
        self.validation_results["quality_checks"]["completeness"] = completeness
    
    def _validate_data_consistency(self):
        """æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥"""
        print("\nğŸ” Data Consistency Check:")
        print("-" * 40)
        
        # æ£€æŸ¥æ—¶é—´é€»è¾‘ä¸€è‡´æ€§
        # æ³¨æ„ï¼šåœ¨ hsp_service_details ä¸­ï¼Œæ¯æ¡è®°å½•ä»£è¡¨ä¸€ä¸ªç«™ç‚¹
        # å¯¹äºä¸­é—´ç«™ï¼Œåˆ°è¾¾æ—¶é—´ < å‡ºå‘æ—¶é—´æ˜¯æ­£å¸¸çš„ï¼ˆåˆ—è½¦å…ˆåˆ°è¾¾ï¼Œç„¶åå‡ºå‘ï¼‰
        # æˆ‘ä»¬éœ€è¦æ£€æŸ¥çš„æ˜¯ï¼šåŒä¸€ç«™ç‚¹å†…ï¼Œå‡ºå‘æ—¶é—´åº”è¯¥ >= åˆ°è¾¾æ—¶é—´ï¼ˆå¦‚æœä¸¤è€…éƒ½å­˜åœ¨ï¼‰
        # æˆ–è€…æ£€æŸ¥è·¨ç«™ç‚¹çš„æ—¶é—´é¡ºåºï¼ˆè¿™éœ€è¦æ›´å¤æ‚çš„é€»è¾‘ï¼‰
        
        # æ£€æŸ¥åŒä¸€ç«™ç‚¹å†…çš„æ—¶é—´é€»è¾‘ï¼šå‡ºå‘æ—¶é—´åº”è¯¥ >= åˆ°è¾¾æ—¶é—´ï¼ˆå¦‚æœä¸¤è€…éƒ½å­˜åœ¨ï¼‰
        # å¦‚æœå‡ºå‘æ—¶é—´ < åˆ°è¾¾æ—¶é—´ï¼Œè¿™æ˜¯é”™è¯¯çš„ï¼ˆé™¤éæ˜¯è·¨åˆå¤œï¼Œä½†æˆ‘ä»¬å·²ç»å¤„ç†äº†æ—¥æœŸï¼‰
        inconsistent_times = self.cursor.execute("""
            SELECT COUNT(*) FROM hsp_service_details
            WHERE actual_departure IS NOT NULL 
            AND actual_arrival IS NOT NULL
            AND actual_departure < actual_arrival
            AND CAST(strftime('%s', actual_departure) AS INTEGER) < 
                CAST(strftime('%s', actual_arrival) AS INTEGER) - 60
        """).fetchone()[0]
        
        # æ£€æŸ¥è·¨åˆå¤œçš„æƒ…å†µï¼ˆå‡ºå‘æ—¶é—´ < åˆ°è¾¾æ—¶é—´ï¼Œä½†æ—¶é—´å·® > 12å°æ—¶ï¼Œå¯èƒ½æ˜¯æ—¥æœŸé”™è¯¯ï¼‰
        cross_midnight_errors = self.cursor.execute("""
            SELECT COUNT(*) FROM hsp_service_details
            WHERE actual_departure IS NOT NULL 
            AND actual_arrival IS NOT NULL
            AND actual_departure < actual_arrival
            AND (CAST(strftime('%s', actual_arrival) AS INTEGER) - 
                 CAST(strftime('%s', actual_departure) AS INTEGER)) > 43200
        """).fetchone()[0]
        
        if inconsistent_times > 0:
            print(f"âŒ Found {inconsistent_times} records with departure < arrival (time gap > 1 min)")
            self.validation_results["errors"].append(
                f"Time inconsistency: {inconsistent_times} records with departure < arrival"
            )
        else:
            print("âœ… All time sequences are consistent")
        
        if cross_midnight_errors > 0:
            print(f"âš ï¸ Found {cross_midnight_errors} records with potential cross-midnight date issues")
            self.validation_results["warnings"].append(
                f"Potential cross-midnight date issues: {cross_midnight_errors} records"
            )
        
        # æ£€æŸ¥å»¶è¯¯è®¡ç®—ä¸€è‡´æ€§
        delay_check = self.cursor.execute("""
            SELECT COUNT(*) FROM hsp_service_details
            WHERE scheduled_arrival IS NOT NULL
            AND actual_arrival IS NOT NULL
            AND arrival_delay_minutes IS NOT NULL
            AND ABS(
                (CAST(strftime('%s', actual_arrival) AS INTEGER) - 
                 CAST(strftime('%s', scheduled_arrival) AS INTEGER)) / 60
                - arrival_delay_minutes
            ) > 2
        """).fetchone()[0]
        
        if delay_check > 0:
            print(f"âš ï¸ Found {delay_check} records with inconsistent delay calculations")
            self.validation_results["warnings"].append(
                f"Delay calculation inconsistency: {delay_check} records"
            )
        else:
            print("âœ… Delay calculations are consistent")
        
        self.validation_results["quality_checks"]["consistency"] = {
            "time_sequence_errors": inconsistent_times,
            "delay_calculation_errors": delay_check
        }
    
    def _validate_delay_quality(self):
        """å»¶è¯¯æ•°æ®è´¨é‡åˆ†æï¼ˆORRæ ‡å‡†ï¼‰"""
        print("\nğŸ“ˆ Delay Data Quality (ORR Standards):")
        print("-" * 40)
        
        # ä½¿ç”¨SQLèšåˆè®¡ç®—ï¼Œé¿å…åŠ è½½æ‰€æœ‰è®°å½•åˆ°å†…å­˜
        # æ³¨æ„ï¼šcancellation_reason æ˜¯ location çº§åˆ«çš„ï¼Œä¸æ˜¯æœåŠ¡çº§åˆ«çš„
        # åªæœ‰å½“æ•´ä¸ªæœåŠ¡éƒ½æ²¡æœ‰å®é™…æ—¶é—´æ—¶ï¼Œæ‰ç®—ä½œå–æ¶ˆ
        
        # è·å–æ€»è®°å½•æ•°
        total_all = self.cursor.execute("""
            SELECT COUNT(*) FROM hsp_service_details
        """).fetchone()[0]
        
        if total_all == 0:
            print("âš ï¸ No delay records found")
            return
        
        # è®¡ç®—å–æ¶ˆè®°å½•æ•°ï¼ˆæ²¡æœ‰å®é™…æ—¶é—´å’Œå»¶è¿Ÿæ•°æ®ï¼‰
        cancelled_count = self.cursor.execute("""
            SELECT COUNT(*) FROM hsp_service_details
            WHERE actual_departure IS NULL 
            AND actual_arrival IS NULL 
            AND arrival_delay_minutes IS NULL
        """).fetchone()[0]
        
        # è·å–æœ‰å»¶è¿Ÿæ•°æ®çš„è®°å½•æ•°
        total_with_delay = self.cursor.execute("""
            SELECT COUNT(*) FROM hsp_service_details
            WHERE arrival_delay_minutes IS NOT NULL
        """).fetchone()[0]
        
        if total_with_delay == 0:
            print("âš ï¸ No delay data available for analysis")
            return
        
        # ä½¿ç”¨SQLè®¡ç®—åŸºç¡€ç»Ÿè®¡ï¼ˆå‡å€¼ï¼‰
        mean_delay = self.cursor.execute("""
            SELECT AVG(arrival_delay_minutes)
            FROM hsp_service_details
            WHERE arrival_delay_minutes IS NOT NULL
        """).fetchone()[0] or 0
        
        # è®¡ç®—ä¸­ä½æ•°ï¼ˆä½¿ç”¨OFFSETæ–¹æ³•ï¼Œå¯¹äºå¤§æ•°æ®é›†å¯èƒ½è¾ƒæ…¢ä½†æ›´å‡†ç¡®ï¼‰
        # å…ˆè·å–æ€»æ•°
        delay_count = total_with_delay
        offset = delay_count // 2
        
        median_result = self.cursor.execute("""
            SELECT arrival_delay_minutes
            FROM hsp_service_details
            WHERE arrival_delay_minutes IS NOT NULL
            ORDER BY arrival_delay_minutes
            LIMIT 1 OFFSET ?
        """, (offset,)).fetchone()
        median_delay = median_result[0] if median_result else 0
        
        # è®¡ç®—æ ‡å‡†å·®
        variance = self.cursor.execute("""
            SELECT AVG((arrival_delay_minutes - ?) * (arrival_delay_minutes - ?))
            FROM hsp_service_details
            WHERE arrival_delay_minutes IS NOT NULL
        """, (mean_delay, mean_delay)).fetchone()[0]
        stdev_delay = (variance ** 0.5) if variance is not None and variance >= 0 else 0
        
        # ORR æ ‡å‡†åˆ†å¸ƒç»Ÿè®¡ï¼ˆä½¿ç”¨SQLèšåˆï¼‰
        orr_stats = self.cursor.execute("""
            SELECT 
                SUM(CASE WHEN arrival_delay_minutes <= 1 THEN 1 ELSE 0 END) as on_time,
                SUM(CASE WHEN arrival_delay_minutes <= 3 THEN 1 ELSE 0 END) as time_to_3,
                SUM(CASE WHEN arrival_delay_minutes <= 15 THEN 1 ELSE 0 END) as time_to_15,
                SUM(CASE WHEN arrival_delay_minutes <= 30 THEN 1 ELSE 0 END) as time_to_30,
                SUM(CASE WHEN arrival_delay_minutes <= 60 THEN 1 ELSE 0 END) as time_to_60,
                SUM(CASE WHEN arrival_delay_minutes > 60 THEN 1 ELSE 0 END) as over_60
            FROM hsp_service_details
            WHERE arrival_delay_minutes IS NOT NULL
        """).fetchone()
        
        on_time = orr_stats[0] or 0
        time_to_3 = orr_stats[1] or 0
        time_to_15 = orr_stats[2] or 0
        time_to_30 = orr_stats[3] or 0
        time_to_60 = orr_stats[4] or 0
        over_60 = orr_stats[5] or 0
        
        # è®¡ç®—ç™¾åˆ†æ¯”ï¼ˆåŸºäºæ‰€æœ‰éå–æ¶ˆè®°å½•ï¼‰
        print(f"ğŸ“Š Total records: {total_all:,}")
        if cancelled_count > 0:
            print(f"ğŸš« Cancelled services: {cancelled_count:,} ({cancelled_count/total_all*100:.1f}%)")
        print(f"ğŸ“Š Records with delay data: {total_with_delay:,}")
        print(f"ğŸ“Š Mean delay: {mean_delay:.1f} minutes")
        print(f"ğŸ“Š Median delay: {median_delay:.1f} minutes")
        print(f"ğŸ“Š Std deviation: {stdev_delay:.1f} minutes")
        print(f"\nğŸ“Š ORR Performance Metrics:")
        print(f"  âœ… On Time (â‰¤1 min): {on_time:,} ({on_time/total_with_delay*100:.1f}%)")
        print(f"  â±ï¸  Time to 3 min (â‰¤3 min): {time_to_3:,} ({time_to_3/total_with_delay*100:.1f}%)")
        print(f"  â±ï¸  Time to 15 min (â‰¤15 min): {time_to_15:,} ({time_to_15/total_with_delay*100:.1f}%)")
        print(f"  â±ï¸  Time to 30 min (â‰¤30 min): {time_to_30:,} ({time_to_30/total_with_delay*100:.1f}%)")
        print(f"  â±ï¸  Time to 60 min (â‰¤60 min): {time_to_60:,} ({time_to_60/total_with_delay*100:.1f}%)")
        print(f"  âŒ Over 60 min: {over_60:,} ({over_60/total_with_delay*100:.1f}%)")
        
        if cancelled_count > 0:
            print(f"  ğŸš« Cancelled: {cancelled_count:,} ({cancelled_count/total_all*100:.1f}%)")
        
        # æç«¯å€¼æ£€æµ‹
        if over_60 > 0:
            print(f"\nâš ï¸ Found {over_60} extreme delays (>60 min)")
            self.validation_results["warnings"].append(
                f"Extreme delays detected: {over_60} records"
            )
        
        self.validation_results["quality_checks"]["delay_quality"] = {
            "total_records": total_all,
            "cancelled_count": cancelled_count,
            "records_with_delay": total_with_delay,
            "mean_delay": round(mean_delay, 2),
            "median_delay": round(median_delay, 2),
            "std_deviation": round(stdev_delay, 2),
            "orr_metrics": {
                "on_time": on_time,
                "on_time_rate": round(on_time/total_with_delay*100, 2) if total_with_delay > 0 else 0,
                "time_to_3": time_to_3,
                "time_to_3_rate": round(time_to_3/total_with_delay*100, 2) if total_with_delay > 0 else 0,
                "time_to_15": time_to_15,
                "time_to_15_rate": round(time_to_15/total_with_delay*100, 2) if total_with_delay > 0 else 0,
                "time_to_30": time_to_30,
                "time_to_30_rate": round(time_to_30/total_with_delay*100, 2) if total_with_delay > 0 else 0,
                "time_to_60": time_to_60,
                "time_to_60_rate": round(time_to_60/total_with_delay*100, 2) if total_with_delay > 0 else 0,
                "over_60": over_60,
                "over_60_rate": round(over_60/total_with_delay*100, 2) if total_with_delay > 0 else 0
            },
            "cancelled_rate": round(cancelled_count/total_all*100, 2) if total_all > 0 else 0
        }
    
    def _validate_ppm(self):
        """è®¡ç®— PPM (Public Performance Measure) - ç»ˆç‚¹ç«™å‡†ç‚¹ç‡"""
        print("\nğŸ¯ PPM (Public Performance Measure) Analysis:")
        print("-" * 40)
        
        # è·å–æ¯ä¸ªæœåŠ¡çš„ç»ˆç‚¹ç«™ï¼ˆæœ€åä¸€ä¸ª locationï¼ŒæŒ‰ scheduled_arrival æ’åºï¼‰
        terminal_stations = self.cursor.execute("""
            WITH service_terminals AS (
                SELECT 
                    rid,
                    location,
                    arrival_delay_minutes,
                    scheduled_arrival,
                    actual_arrival,
                    actual_departure,
                    actual_arrival as actual_arr,
                    cancellation_reason,
                    ROW_NUMBER() OVER (
                        PARTITION BY rid 
                        ORDER BY scheduled_arrival DESC
                    ) as rn
                FROM hsp_service_details
                WHERE scheduled_arrival IS NOT NULL
            )
            SELECT 
                rid,
                location,
                arrival_delay_minutes,
                scheduled_arrival,
                actual_arrival,
                cancellation_reason
            FROM service_terminals
            WHERE rn = 1
        """).fetchall()
        
        if not terminal_stations:
            print("âš ï¸ No terminal station data found")
            return
        
        total_services = len(terminal_stations)
        
        # æ’é™¤å–æ¶ˆçš„æœåŠ¡ï¼ˆæ²¡æœ‰å®é™…åˆ°è¾¾æ—¶é—´ï¼‰
        valid_services = [
            s for s in terminal_stations
            if s['actual_arrival'] is not None
        ]
        
        cancelled_services = total_services - len(valid_services)
        
        # è®¡ç®— PPM-5 (5åˆ†é’Ÿå†…) å’Œ PPM-10 (10åˆ†é’Ÿå†…)
        ppm_5_count = sum(
            1 for s in valid_services
            if s['arrival_delay_minutes'] is not None and s['arrival_delay_minutes'] <= 5
        )
        ppm_10_count = sum(
            1 for s in valid_services
            if s['arrival_delay_minutes'] is not None and s['arrival_delay_minutes'] <= 10
        )
        
        # è®¡ç®—ç™¾åˆ†æ¯”ï¼ˆåŸºäºæ‰€æœ‰æœ‰æ•ˆæœåŠ¡ï¼Œä¸åŒ…æ‹¬å–æ¶ˆçš„ï¼‰
        valid_count = len(valid_services)
        ppm_5_rate = (ppm_5_count / valid_count * 100) if valid_count > 0 else 0
        ppm_10_rate = (ppm_10_count / valid_count * 100) if valid_count > 0 else 0
        
        print(f"ğŸ“Š Total services: {total_services:,}")
        if cancelled_services > 0:
            print(f"ğŸš« Cancelled services: {cancelled_services:,} ({cancelled_services/total_services*100:.1f}%)")
        print(f"ğŸ“Š Valid services (with arrival data): {valid_count:,}")
        print(f"\nğŸ“Š PPM Metrics:")
        print(f"  âœ… PPM-5 (â‰¤5 min): {ppm_5_count:,} ({ppm_5_rate:.1f}%)")
        print(f"  âœ… PPM-10 (â‰¤10 min): {ppm_10_count:,} ({ppm_10_rate:.1f}%)")
        
        # æŒ‰ TOC åˆ†ç»„è®¡ç®— PPM
        print(f"\nğŸ“Š PPM by TOC:")
        toc_ppm = self.cursor.execute("""
            WITH service_terminals AS (
                SELECT 
                    d.rid,
                    d.toc_code,
                    d.location,
                    d.arrival_delay_minutes,
                    d.actual_arrival,
                    ROW_NUMBER() OVER (
                        PARTITION BY d.rid 
                        ORDER BY d.scheduled_arrival DESC
                    ) as rn
                FROM hsp_service_details d
                WHERE d.scheduled_arrival IS NOT NULL
            )
            SELECT 
                toc_code,
                COUNT(*) as total_services,
                SUM(CASE WHEN actual_arrival IS NULL THEN 1 ELSE 0 END) as cancelled,
                SUM(CASE WHEN actual_arrival IS NOT NULL AND arrival_delay_minutes <= 5 THEN 1 ELSE 0 END) as ppm_5,
                SUM(CASE WHEN actual_arrival IS NOT NULL AND arrival_delay_minutes <= 10 THEN 1 ELSE 0 END) as ppm_10,
                COUNT(CASE WHEN actual_arrival IS NOT NULL THEN 1 END) as valid_services
            FROM service_terminals
            WHERE rn = 1
            GROUP BY toc_code
            ORDER BY total_services DESC
            LIMIT 10
        """).fetchall()
        
        for toc, total, cancelled, ppm5, ppm10, valid in toc_ppm:
            if valid > 0:
                ppm5_rate = (ppm5 / valid * 100)
                ppm10_rate = (ppm10 / valid * 100)
                cancelled_str = f", {cancelled:,} cancelled" if cancelled > 0 else ""
                print(f"  {toc}: {total:,} services{cancelled_str}, PPM-5: {ppm5_rate:.1f}%, PPM-10: {ppm10_rate:.1f}%")
        
        # ä¿å­˜åˆ°ç»“æœ
        self.validation_results["quality_checks"]["ppm"] = {
            "total_services": total_services,
            "cancelled_services": cancelled_services,
            "valid_services": valid_count,
            "ppm_5": {
                "count": ppm_5_count,
                "rate": round(ppm_5_rate, 2)
            },
            "ppm_10": {
                "count": ppm_10_count,
                "rate": round(ppm_10_rate, 2)
            },
            "toc_breakdown": {
                toc: {
                    "total_services": total,
                    "cancelled": cancelled,
                    "valid_services": valid,
                    "ppm_5_rate": round((ppm5 / valid * 100), 2) if valid > 0 else 0,
                    "ppm_10_rate": round((ppm10 / valid * 100), 2) if valid > 0 else 0
                }
                for toc, total, cancelled, ppm5, ppm10, valid in toc_ppm
            }
        }
    
    def _validate_route_coverage(self):
        """è·¯çº¿è¦†ç›–æ£€æŸ¥"""
        print("\nğŸ›¤ï¸ Route Coverage Analysis:")
        print("-" * 40)
        
        # é¢„æœŸçš„10æ¡è·¯çº¿ï¼ˆä½¿ç”¨æ­£ç¡®çš„è½¦ç«™ä»£ç ï¼‰
        # æ³¨æ„ï¼šå·²ä¿®å¤çš„è·¯çº¿ï¼ˆMYB-BHMâ†’EUS-BHM, MAN-LIVâ†’MCV-LIV, MAN-LDSâ†’MCV-LDSï¼‰
        expected_routes = [
            ("EUS", "MAN"),  # London Euston â†’ Manchester
            ("KGX", "EDB"),  # King's Cross â†’ Edinburgh (EDB, not EDR)
            ("PAD", "BRI"),  # Paddington â†’ Bristol
            ("LST", "NRW"),  # Liverpool St â†’ Norwich
            ("EUS", "BHM"),  # London Euston â†’ Birmingham (replaces MYB-BHM)
            ("MCV", "LIV"),  # Manchester Victoria â†’ Liverpool (replaces MAN-LIV)
            ("BHM", "MAN"),  # Birmingham â†’ Manchester
            ("BRI", "BHM"),  # Bristol â†’ Birmingham
            ("EDB", "GLC"),  # Edinburgh â†’ Glasgow (EDB, not EDR)
            ("MCV", "LDS")   # Manchester Victoria â†’ Leeds (replaces MAN-LDS)
        ]
        
        # æ£€æŸ¥æ¯æ¡è·¯çº¿çš„æ•°æ®é‡
        route_coverage = {}
        missing_routes = []
        low_data_routes = []
        
        for origin, destination in expected_routes:
            # Query from hsp_service_metrics which has origin and destination
            count = self.cursor.execute("""
                SELECT COUNT(*) 
                FROM hsp_service_metrics
                WHERE origin = ? AND destination = ?
            """, (origin, destination)).fetchone()[0]
            
            route_name = f"{origin}-{destination}"
            route_coverage[route_name] = count
            
            if count == 0:
                missing_routes.append(route_name)
                print(f"âŒ {route_name}: No data")
            elif count < 100:
                low_data_routes.append(route_name)
                print(f"âš ï¸ {route_name}: {count} services (low)")
            else:
                print(f"âœ… {route_name}: {count} services")
        
        if missing_routes:
            self.validation_results["errors"].append(
                f"Missing routes: {', '.join(missing_routes)}"
            )
        
        if low_data_routes:
            self.validation_results["warnings"].append(
                f"Low data routes: {', '.join(low_data_routes)}"
            )
        
        self.validation_results["quality_checks"]["route_coverage"] = route_coverage
        
        # è¦†ç›–ç‡ç»Ÿè®¡
        covered_routes = len([r for r in route_coverage.values() if r > 0])
        coverage_rate = (covered_routes / len(expected_routes)) * 100
        print(f"\nğŸ“Š Route coverage: {covered_routes}/{len(expected_routes)} ({coverage_rate:.0f}%)")
    
    def _validate_temporal_distribution(self):
        """æ—¶é—´åˆ†å¸ƒæ£€æŸ¥"""
        print("\nğŸ“… Temporal Distribution:")
        print("-" * 40)
        
        # æŒ‰æ—¥æœŸåˆ†å¸ƒ
        daily_distribution = self.cursor.execute("""
            SELECT date_of_service, COUNT(*) as count
            FROM hsp_service_details
            GROUP BY date_of_service
            ORDER BY date_of_service
        """).fetchall()
        
        if daily_distribution:
            dates = [d[0] for d in daily_distribution]
            counts = [d[1] for d in daily_distribution]
            
            avg_daily = statistics.mean(counts)
            min_daily = min(counts)
            max_daily = max(counts)
            
            print(f"ğŸ“Š Date range: {dates[0]} to {dates[-1]}")
            print(f"ğŸ“Š Total days: {len(dates)}")
            print(f"ğŸ“Š Avg records/day: {avg_daily:.0f}")
            print(f"ğŸ“Š Min records/day: {min_daily}")
            print(f"ğŸ“Š Max records/day: {max_daily}")
            
            # æ£€æŸ¥æ•°æ®ç©ºç¼º
            from datetime import datetime
            date_set = set(dates)
            start_date = datetime.strptime(dates[0], "%Y-%m-%d")
            end_date = datetime.strptime(dates[-1], "%Y-%m-%d")
            
            expected_days = (end_date - start_date).days + 1
            missing_days = expected_days - len(dates)
            
            if missing_days > 0:
                print(f"âš ï¸ Missing data for {missing_days} days")
                self.validation_results["warnings"].append(
                    f"Missing data for {missing_days} days"
                )
            
            # æŒ‰æ˜ŸæœŸåˆ†å¸ƒ
            weekday_dist = self.cursor.execute("""
                SELECT 
                    CASE cast(strftime('%w', date_of_service) as integer)
                        WHEN 0 THEN 'Sunday'
                        WHEN 1 THEN 'Monday'
                        WHEN 2 THEN 'Tuesday'
                        WHEN 3 THEN 'Wednesday'
                        WHEN 4 THEN 'Thursday'
                        WHEN 5 THEN 'Friday'
                        WHEN 6 THEN 'Saturday'
                    END as weekday,
                    COUNT(*) as count
                FROM hsp_service_details
                GROUP BY weekday
                ORDER BY 
                    CASE weekday
                        WHEN 'Monday' THEN 1
                        WHEN 'Tuesday' THEN 2
                        WHEN 'Wednesday' THEN 3
                        WHEN 'Thursday' THEN 4
                        WHEN 'Friday' THEN 5
                        WHEN 'Saturday' THEN 6
                        WHEN 'Sunday' THEN 7
                    END
            """).fetchall()
            
            print("\nğŸ“Š Weekday distribution:")
            for day, count in weekday_dist:
                print(f"  {day}: {count:,} records")
            
            self.validation_results["quality_checks"]["temporal_distribution"] = {
                "total_days": len(dates),
                "missing_days": missing_days,
                "avg_daily_records": round(avg_daily, 0),
                "weekday_distribution": dict(weekday_dist)
            }
    
    def _validate_toc_data(self):
        """TOCæ•°æ®éªŒè¯"""
        print("\nğŸš‚ TOC (Train Operating Company) Analysis:")
        print("-" * 40)
        
        # TOC ç»Ÿè®¡ï¼ˆä½¿ç”¨ ORR æ ‡å‡†ï¼šâ‰¤1 min ä¸ºå‡†ç‚¹ï¼‰
        # æ³¨æ„ï¼šcancellation_reason æ˜¯ location çº§åˆ«çš„ï¼Œä¸æ˜¯æœåŠ¡çº§åˆ«çš„
        # åªæœ‰å®Œå…¨æ²¡æœ‰å®é™…æ—¶é—´çš„è®°å½•æ‰ç®—å–æ¶ˆ
        toc_stats = self.cursor.execute("""
            SELECT 
                toc_code,
                COUNT(*) as total_services,
                SUM(CASE WHEN actual_departure IS NULL AND actual_arrival IS NULL AND arrival_delay_minutes IS NULL THEN 1 ELSE 0 END) as cancelled,
                COUNT(CASE WHEN arrival_delay_minutes IS NOT NULL THEN 1 END) as services_with_delay,
                AVG(arrival_delay_minutes) as avg_delay,
                SUM(CASE WHEN arrival_delay_minutes <= 1 THEN 1 ELSE 0 END) * 100.0 / 
                    NULLIF(COUNT(CASE WHEN arrival_delay_minutes IS NOT NULL THEN 1 END), 0) as on_time_rate
            FROM hsp_service_details
            GROUP BY toc_code
            ORDER BY total_services DESC
        """).fetchall()
        
        toc_analysis = {}
        for toc, total_services, cancelled, services_with_delay, avg_delay, on_time_rate in toc_stats[:10]:  # Top 10 TOCs
            cancelled_str = f", {cancelled:,} cancelled" if cancelled > 0 else ""
            print(f"  {toc}: {total_services:,} services{cancelled_str}, {avg_delay:.1f}min avg delay, {on_time_rate:.1f}% on-time (â‰¤1 min)")
            toc_analysis[toc] = {
                "total_services": total_services,
                "cancelled": cancelled,
                "services_with_delay": services_with_delay,
                "avg_delay": round(avg_delay, 2) if avg_delay else None,
                "on_time_rate": round(on_time_rate, 2) if on_time_rate else None
            }
        
        self.validation_results["quality_checks"]["toc_analysis"] = toc_analysis
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªçŸ¥TOC
        unknown_tocs = self.cursor.execute("""
            SELECT DISTINCT toc_code
            FROM hsp_service_details
            WHERE toc_code IS NULL OR toc_code = ''
        """).fetchall()
        
        if unknown_tocs:
            print(f"\nâš ï¸ Found {len(unknown_tocs)} records with unknown TOC")
            self.validation_results["warnings"].append(
                f"Unknown TOC codes: {len(unknown_tocs)} records"
            )
    
    def _detect_anomalies(self):
        """å¼‚å¸¸å€¼æ£€æµ‹"""
        print("\nğŸ” Anomaly Detection:")
        print("-" * 40)
        
        anomalies = []
        
        # 1. æç«¯å»¶è¯¯ (>180åˆ†é’Ÿ)
        extreme_delays = self.cursor.execute("""
            SELECT COUNT(*) FROM hsp_service_details
            WHERE ABS(arrival_delay_minutes) > 180
        """).fetchone()[0]
        
        if extreme_delays > 0:
            print(f"âš ï¸ Extreme delays (>3 hours): {extreme_delays} records")
            anomalies.append(f"Extreme delays: {extreme_delays}")
        
        # 2. é‡å¤è®°å½•
        duplicates = self.cursor.execute("""
            SELECT rid, location, COUNT(*) as count
            FROM hsp_service_details
            GROUP BY rid, location
            HAVING count > 1
        """).fetchall()
        
        if duplicates:
            print(f"âš ï¸ Duplicate records: {len(duplicates)} combinations")
            anomalies.append(f"Duplicates: {len(duplicates)}")
        
        # 3. æœªæ¥æ—¥æœŸ
        future_dates = self.cursor.execute("""
            SELECT COUNT(*) FROM hsp_service_details
            WHERE date_of_service > date('now')
        """).fetchone()[0]
        
        if future_dates > 0:
            print(f"âŒ Future dates: {future_dates} records")
            anomalies.append(f"Future dates: {future_dates}")
            self.validation_results["errors"].append(
                f"Data contains future dates: {future_dates} records"
            )
        
        # 4. æ— æ•ˆCRSä»£ç ï¼ˆé3å­—æ¯ï¼‰
        invalid_crs = self.cursor.execute("""
            SELECT COUNT(*) FROM hsp_service_details
            WHERE LENGTH(location) != 3
        """).fetchone()[0]
        
        if invalid_crs > 0:
            print(f"âš ï¸ Invalid CRS codes: {invalid_crs} records")
            anomalies.append(f"Invalid CRS: {invalid_crs}")
        
        if not anomalies:
            print("âœ… No significant anomalies detected")
        
        self.validation_results["quality_checks"]["anomalies"] = anomalies
    
    def _validate_data_freshness(self):
        """æ•°æ®æ–°é²œåº¦æ£€æŸ¥"""
        print("\nğŸ• Data Freshness Check:")
        print("-" * 40)
        
        # æœåŠ¡æ—¥æœŸèŒƒå›´ï¼ˆå®é™…æ•°æ®çš„æ—¶é—´èŒƒå›´ï¼‰
        service_date_range = self.cursor.execute("""
            SELECT MIN(date_of_service) as min_date,
                   MAX(date_of_service) as max_date
            FROM hsp_service_details
        """).fetchone()
        
        # æ•°æ®æ”¶é›†æ—¶é—´ï¼ˆfetch_timestampï¼‰
        fetch_time_range = self.cursor.execute("""
            SELECT MAX(fetch_timestamp) as latest,
                   MIN(fetch_timestamp) as earliest
            FROM hsp_service_details
        """).fetchone()
        
        if service_date_range and service_date_range['min_date']:
            print(f"ğŸ“… Service date range: {service_date_range['min_date']} to {service_date_range['max_date']}")
        
        if fetch_time_range and fetch_time_range['latest']:
            latest = datetime.fromisoformat(fetch_time_range['latest'].replace('Z', '+00:00'))
            earliest = datetime.fromisoformat(fetch_time_range['earliest'].replace('Z', '+00:00'))
            now = datetime.now()
            
            age_hours = (now - latest).total_seconds() / 3600
            collection_span = (latest - earliest).total_seconds() / 3600
            
            print(f"ğŸ“Š Latest fetch time: {fetch_time_range['latest']}")
            print(f"ğŸ“Š Earliest fetch time: {fetch_time_range['earliest']}")
            print(f"ğŸ“Š Data age: {age_hours:.1f} hours")
            print(f"ğŸ“Š Collection span: {collection_span:.1f} hours")
            
            if age_hours < 24:
                print("âœ… Data is fresh (< 24 hours old)")
            elif age_hours < 72:
                print("âš ï¸ Data is relatively fresh (< 72 hours old)")
            else:
                print("âŒ Data is stale (> 72 hours old)")
                self.validation_results["warnings"].append(
                    f"Stale data: {age_hours:.1f} hours old"
                )
            
            freshness_data = {
                "service_date_range": {
                    "min": service_date_range['min_date'] if service_date_range and service_date_range['min_date'] else None,
                    "max": service_date_range['max_date'] if service_date_range and service_date_range['max_date'] else None
                },
                "fetch_time_range": {
                    "latest": fetch_time_range['latest'],
                    "earliest": fetch_time_range['earliest']
                },
                "age_hours": round(age_hours, 1),
                "collection_span_hours": round(collection_span, 1)
            }
            self.validation_results["quality_checks"]["freshness"] = freshness_data
    
    def _generate_recommendations(self):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        print("\nğŸ’¡ Recommendations:")
        print("-" * 40)
        
        recommendations = []
        
        # åŸºäºéªŒè¯ç»“æœç”Ÿæˆå»ºè®®
        if self.validation_results["summary"]["details_records"] < 10000:
            recommendations.append(
                "ğŸ“ˆ Collect more data: Current volume is below 10,000 records target"
            )
        
        if self.validation_results["summary"]["unique_routes"] < 10:
            recommendations.append(
                "ğŸ›¤ï¸ Expand route coverage: Less than 10 routes have data"
            )
        
        completeness = self.validation_results.get("quality_checks", {}).get("completeness", {})
        for field, stats in completeness.items():
            if stats.get("percentage", 100) < 50:
                recommendations.append(
                    f"ğŸ”§ Improve {field} data collection: Only {stats['percentage']}% complete"
                )
        
        if len(self.validation_results.get("errors", [])) > 0:
            recommendations.append(
                "âŒ Address critical errors before proceeding with modeling"
            )
        
        if len(self.validation_results.get("warnings", [])) > 3:
            recommendations.append(
                "âš ï¸ Review and address data quality warnings"
            )
        
        # é¢å¤–å»ºè®®
        recommendations.extend([
            "ğŸ“Š Consider collecting weekend data for better coverage",
            "ğŸ”„ Implement incremental data updates for freshness",
            "ğŸ“ Add metadata collection for TOC and station information",
            "ğŸ¯ Focus on high-traffic routes for initial predictions"
        ])
        
        for i, rec in enumerate(recommendations[:5], 1):  # Top 5 recommendations
            print(f"{i}. {rec}")
        
        self.validation_results["recommendations"] = recommendations
    
    def generate_report(self, output_file: Optional[str] = None) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        report = []
        report.append("=" * 70)
        report.append("DATA VALIDATION REPORT")
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("=" * 70)
        
        # Summary
        report.append("\nğŸ“Š SUMMARY")
        report.append("-" * 40)
        summary = self.validation_results["summary"]
        report.append(f"Total Records: {summary.get('details_records', 0):,}")
        report.append(f"Unique Routes: {summary.get('unique_routes', 0)}")
        report.append(f"Unique Services: {summary.get('unique_services', 0):,}")
        report.append(f"Date Range: {summary.get('date_range', {}).get('start')} to {summary.get('date_range', {}).get('end')}")
        report.append(f"TOCs: {summary.get('toc_count', 0)}")
        report.append(f"Locations: {summary.get('location_count', 0)}")
        
        # Quality Score
        report.append("\nğŸ“ˆ QUALITY SCORE")
        report.append("-" * 40)
        
        # Calculate quality score
        score = 100
        score -= len(self.validation_results.get("errors", [])) * 10
        score -= len(self.validation_results.get("warnings", [])) * 2
        score = max(0, score)
        
        report.append(f"Overall Quality Score: {score}/100")
        report.append(f"Critical Errors: {len(self.validation_results.get('errors', []))}")
        report.append(f"Warnings: {len(self.validation_results.get('warnings', []))}")
        
        # Errors
        if self.validation_results.get("errors"):
            report.append("\nâŒ CRITICAL ERRORS")
            report.append("-" * 40)
            for error in self.validation_results["errors"]:
                report.append(f"â€¢ {error}")
        
        # Warnings
        if self.validation_results.get("warnings"):
            report.append("\nâš ï¸  WARNINGS")
            report.append("-" * 40)
            for warning in self.validation_results["warnings"]:
                report.append(f"â€¢ {warning}")
        
        # Recommendations
        if self.validation_results.get("recommendations"):
            report.append("\nğŸ’¡ RECOMMENDATIONS")
            report.append("-" * 40)
            for i, rec in enumerate(self.validation_results["recommendations"][:5], 1):
                report.append(f"{i}. {rec}")
        
        # Success Criteria
        report.append("\nâœ… WEEK 1 SUCCESS CRITERIA")
        report.append("-" * 40)
        
        criteria_met = 0
        criteria_total = 3
        
        if summary.get("details_records", 0) >= 10000:
            report.append("âœ… Data volume: â‰¥10,000 records")
            criteria_met += 1
        else:
            report.append(f"âŒ Data volume: {summary.get('details_records', 0):,} < 10,000 records")
        
        if summary.get("unique_routes", 0) >= 10:
            report.append("âœ… Route coverage: â‰¥10 routes")
            criteria_met += 1
        else:
            report.append(f"âŒ Route coverage: {summary.get('unique_routes', 0)} < 10 routes")
        
        if score >= 70:
            report.append(f"âœ… Quality validation: {score}/100 â‰¥ 70")
            criteria_met += 1
        else:
            report.append(f"âŒ Quality validation: {score}/100 < 70")
        
        report.append(f"\nğŸ“Š Success Rate: {criteria_met}/{criteria_total} ({criteria_met/criteria_total*100:.0f}%)")
        
        report_text = "\n".join(report)
        
        if output_file:
            with open(output_file, 'w') as f:
                f.write(report_text)
            print(f"\nğŸ“„ Report saved to: {output_file}")
        
        return report_text
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        self.conn.close()


def main():
    parser = argparse.ArgumentParser(description="Validate collected HSP data")
    parser.add_argument(
        "--db",
        default="data/railfair.db",
        help="Path to SQLite database"
    )
    parser.add_argument(
        "--output",
        default="data/validation_report.txt",
        help="Output report file"
    )
    parser.add_argument(
        "--json",
        default="data/validation_results.json",
        help="Output JSON file"
    )
    
    args = parser.parse_args()
    
    # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
    Path("data").mkdir(exist_ok=True)
    
    # è¿è¡ŒéªŒè¯
    validator = DataValidator(args.db)
    
    try:
        results = validator.run_all_validations()
        report = validator.generate_report(args.output)
        print("\n" + "=" * 60)
        print("ğŸ“‹ VALIDATION COMPLETE")
        print("=" * 60)
        
        # æ‰“å°æ€»ç»“
        score = 100 - len(results.get("errors", [])) * 10 - len(results.get("warnings", [])) * 2
        score = max(0, score)
        
        if score >= 80:
            print(f"âœ… Data quality is GOOD ({score}/100)")
        elif score >= 60:
            print(f"âš ï¸ Data quality is ACCEPTABLE ({score}/100)")
        else:
            print(f"âŒ Data quality is POOR ({score}/100)")
        
        # ä¿å­˜ JSON æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šäº†ï¼‰
        if args.json:
            import json
            with open(args.json, 'w') as f:
                json.dump(validator.validation_results, f, indent=2, default=str)
            print(f"\nğŸ“„ Full report saved to: {args.output}")
            print(f"ğŸ“Š JSON results saved to: {args.json}")
        else:
            print(f"\nğŸ“„ Full report saved to: {args.output}")
        
    finally:
        validator.close()


if __name__ == "__main__":
    main()
